# Copy to .env and fill values (do NOT commit real secrets).

# Global/default LLM provider and model
LLM_PROVIDER=groq            # fallback provider: groq | openai | ollama | gemini
LLM_MODEL=openai/gpt-oss-20b # fallback model string

# Groq
GROQ_API_KEY=your_groq_api_key_here

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
# Optional: custom OpenAI base (e.g., for org/on-prem deployments)
OPENAI_API_BASE=

# Ollama (local or hosted)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_KEY=

# Gemini (example external endpoint)
GEMINI_API_KEY=
GEMINI_API_URL=

# Output / file settings
BASE_OUTPUT_DIR=books_output
INTERMEDIATE_CSV_FILENAME=intermediate_data_validated.csv
FINAL_CSV_FILENAME=final_image_descriptions.csv

# Optional per-node overrides (alternative to editing LLM_NODE_CONFIG in code)
# Use lowercase provider names: groq, openai, ollama, gemini
# Example usage:
# NODE_EXTRACT_CAPTION_PROVIDER=ollama
# NODE_EXTRACT_CAPTION_MODEL=gemma3:4b
# NODE_EXTRACT_DESCRIPTION_PROVIDER=ollama
# NODE_EXTRACT_DESCRIPTION_MODEL=gemma3:4b
# NODE_VALIDATE_CONTENT_PROVIDER=ollama
# NODE_VALIDATE_CONTENT_MODEL=gemma3:4b
# NODE_CLEAN_DESCRIPTION_PROVIDER=ollama
# NODE_CLEAN_DESCRIPTION_MODEL=gemma3:4b

# Notes:
# - Replace placeholder keys with real API keys for your providers.
# - Keep this file out of version control when it contains real secrets (use .gitignore).
# - If using Ollama locally, ensure the service is running and reachable by OLLAMA_BASE_URL.
